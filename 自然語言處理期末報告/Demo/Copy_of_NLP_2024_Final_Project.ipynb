{"cells":[{"cell_type":"markdown","metadata":{"id":"A_e9oz5Z_pLs"},"source":["# Final Project of the NLP 2024 Course\n","\n","Slides: https://docs.google.com/presentation/d/1NbH4E2HKVHQlaW_ivKCyjpWuEJFvmz3bSKsX8fs67tA/edit#slide=id.g2d17364e0e4_0_34\n"]},{"cell_type":"markdown","metadata":{"id":"ntG5sC-XvQk8"},"source":["## Environment Setup\n","\n","Get your own huggingface access token via\n","https://huggingface.co/settings/tokens\n","\n","And set up HF_TOKEN as a secret of Colab"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"OFoUUaYH6tZz","outputId":"8e38817f-f412-416c-cd0f-9a45d358b4df","executionInfo":{"status":"ok","timestamp":1718242263941,"user_tz":-480,"elapsed":82177,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Collecting accelerate\n","  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install transformers accelerate"]},{"cell_type":"markdown","metadata":{"id":"R5t5Dtt-6EDk"},"source":["## Using the pre-trained model"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9LhK4If5cnSf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718242311898,"user_tz":-480,"elapsed":47999,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"outputId":"827e4beb-d62a-42ac-ff82-6c178918964e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yD5FStxrjgBX","executionInfo":{"status":"ok","timestamp":1718242381505,"user_tz":-480,"elapsed":41181,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from google.colab import userdata\n","\n","driveDir = '/content/drive/MyDrive/1122自然語言處理/自然語言處理期末報告/Model/fine-tuned-model'\n","\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(driveDir, trust_remote_code=True, token=userdata.get('HF_TOKEN'))\n","model = AutoModelForCausalLM.from_pretrained(driveDir, trust_remote_code=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Di_LT8RnhuQm","outputId":"573567a3-12d1-4a8e-fd3b-8b64bfee171e","executionInfo":{"status":"ok","timestamp":1718242392472,"user_tz":-480,"elapsed":522,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["222\n","222\n"]}],"source":["if isinstance(model, str):\n","  print(111)\n","else:\n","  print(222)\n","\n","if isinstance(tokenizer, str):\n","  print(111)\n","else:\n","  print(222)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gfQmSvzWblXM","executionInfo":{"status":"ok","timestamp":1718242443785,"user_tz":-480,"elapsed":762,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[],"source":["\"\"\"Module to generate OpenELM output given a model and an input prompt.\"\"\"\n","import os\n","import logging\n","import time\n","import argparse\n","from typing import Optional, Union\n","import torch\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","from google.colab import userdata\n","\n","\n","# The following function is revised from https://huggingface.co/apple/OpenELM/blob/main/generate_openelm.py\n","def generate(\n","    prompt: str,\n","    model: Union[str, AutoModelForCausalLM],\n","    hf_access_token: str = None,\n","    tokenizer: Union[str, AutoTokenizer] = 'meta-llama/Llama-2-7b-hf',\n","    device: Optional[str] = None,\n","    max_length: int = 1024,\n","    assistant_model: Optional[Union[str, AutoModelForCausalLM]] = None,\n","    generate_kwargs: Optional[dict] = None,\n",") -> str:\n","    \"\"\" Generates output given a prompt.\n","    Args:\n","        prompt: The string prompt.\n","        model: The LLM Model. If a string is passed, it should be the path to\n","            the hf converted checkpoint.\n","        hf_access_token: Hugging face access token.\n","        tokenizer: Tokenizer instance. If model is set as a string path,\n","            the tokenizer will be loaded from the checkpoint.\n","        device: String representation of device to run the model on. If None\n","            and cuda available it would be set to cuda:0 else cpu.\n","        max_length: Maximum length of tokens, input prompt + generated tokens.\n","        assistant_model: If set, this model will be used for\n","            speculative generation. If a string is passed, it should be the\n","            path to the hf converted checkpoint.\n","        generate_kwargs: Extra kwargs passed to the hf generate function.\n","    Returns:\n","        output_text: output generated as a string.\n","        generation_time: generation time in seconds.\n","    Raises:\n","        ValueError: If device is set to CUDA but no CUDA device is detected.\n","        ValueError: If tokenizer is not set.\n","        ValueError: If hf_access_token is not specified.\n","    \"\"\"\n","    if not device:\n","        if torch.cuda.is_available() and torch.cuda.device_count():\n","            device = \"cuda:0\"\n","            logging.warning(\n","                'inference device is not set, using cuda:0, %s',\n","                torch.cuda.get_device_name(0)\n","            )\n","        else:\n","            device = 'cpu'\n","            logging.warning(\n","                (\n","                    'No CUDA device detected, using cpu, '\n","                    'expect slower speeds.'\n","                )\n","            )\n","\n","    if 'cuda' in device and not torch.cuda.is_available():\n","        raise ValueError('CUDA device requested but no CUDA device detected.')\n","\n","    if not tokenizer:\n","        raise ValueError('Tokenizer is not set in the generate function.')\n","\n","    if not hf_access_token:\n","        raise ValueError((\n","            'Hugging face access token needs to be specified. '\n","            'Please refer to https://huggingface.co/docs/hub/security-tokens'\n","            ' to obtain one.'\n","            )\n","        )\n","\n","    if isinstance(model, str):\n","        checkpoint_path = model\n","        model = AutoModelForCausalLM.from_pretrained(\n","            checkpoint_path,\n","            trust_remote_code=True\n","        )\n","    else:\n","        model = model\n","\n","    model.to(device).eval()\n","    if isinstance(tokenizer, str):\n","        tokenizer = AutoTokenizer.from_pretrained(\n","            tokenizer,\n","            token=hf_access_token,\n","        )\n","    else:\n","        tokenizer = tokenizer\n","\n","    # Speculative mode\n","    draft_model = None\n","    if assistant_model:\n","        draft_model = assistant_model\n","        if isinstance(assistant_model, str):\n","            draft_model = AutoModelForCausalLM.from_pretrained(\n","                assistant_model,\n","                trust_remote_code=True\n","            )\n","        draft_model.to(device).eval()\n","\n","    # Prepare the prompt\n","    tokenized_prompt = tokenizer(prompt)\n","    tokenized_prompt = torch.tensor(\n","        tokenized_prompt['input_ids'],\n","        device=device\n","    )\n","\n","    tokenized_prompt = tokenized_prompt.unsqueeze(0)\n","\n","\n","    # Generate\n","    stime = time.time()\n","    output_ids = model.generate(\n","        tokenized_prompt,\n","        max_length=max_length,\n","        pad_token_id=0,\n","        assistant_model=draft_model,\n","        **(generate_kwargs if generate_kwargs else {}),\n","    )\n","    generation_time = time.time() - stime\n","\n","    output_text = tokenizer.decode(\n","        output_ids[0][tokenized_prompt.shape[1]:].tolist(),\n","        skip_special_tokens=True\n","    )\n","\n","    return output_text, generation_time"]},{"cell_type":"markdown","metadata":{"id":"C2upj5yE6I-s"},"source":["## Implement your main function here\n","The input `abstract` is a `str` that forms an abstract of a research paper.\n","Your function will be invoked for returning the **sentence(s)** from the `abstract` that show the **research methodology**."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_PEde7UC6KSL","executionInfo":{"status":"ok","timestamp":1718242462132,"user_tz":-480,"elapsed":815,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[],"source":["def extract_sentence(abstract: str) -> str:\n","    prompt = \"From the following abstract, extract the sentences that shows the methods of the research. Only the sentences from the abstract, no other information.\\n\\n\\n```%s```\" % abstract\n","    output_text, genertaion_time = generate(\n","        prompt=prompt,\n","        # model=\"apple/OpenELM-1_1B-Instruct\",\n","        hf_access_token=userdata.get('HF_TOKEN'),\n","        tokenizer=tokenizer,\n","        model=model,\n","    )\n","    return output_text"]},{"cell_type":"markdown","metadata":{"id":"0Y6hx64u59Z-"},"source":["Your function is expected to be used as follows."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcjRijoPvJWN","outputId":"c42c2641-8652-477c-b266-49993932b243","executionInfo":{"status":"ok","timestamp":1718242490740,"user_tz":-480,"elapsed":27121,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\n","This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\n","We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\n","Furthermore, we apply our model to prune the self-labeled training data.\n","Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\n","\n","\n","```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\n","This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\n","We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\n","Furthermore, we apply our model to prune the self-labeled training data.\n","Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\n","\n","\n","```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\n","This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\n","We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\n","Furthermore, we apply our model to prune the self-labeled training data.\n","Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\n","\n","\n","```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\n","This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\n","We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\n","Furthermore, we apply our model to prune the self-labeled training data.\n","Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\n","\n","\n","```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\n","This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\n","We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\n","Furthermore, we apply our model to prune the self-labeled training data.\n","Experimental results show that\n"]}],"source":["abstract = \"\"\"The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\n","This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\n","We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\n","Furthermore, we apply our model to prune the self-labeled training data.\n","Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.\"\"\"\n","\n","predicted = extract_sentence(abstract)\n","print(predicted)"]},{"cell_type":"markdown","metadata":{"id":"-DorKYCw7U00"},"source":["## Evaluation\n","\n","We will evaluate your module with a close testset.\n","The sentence returned by your function will be compared with a golden reference.\n","The evaluation metric is `ROUGE-L`, which measures the overlap ratio between a predicted output and a reference. The details will be introduced in class."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTJy_hdm5GpA","outputId":"5d0acdcc-093d-4115-d1dc-6819f1048394","executionInfo":{"status":"ok","timestamp":1718242511206,"user_tz":-480,"elapsed":8159,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=bb3a41994a037f5cbefb78af158ac4ac29dc4a59698937395d6a12c68dee9eca\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}],"source":["!pip install rouge-score"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"frSkyu5u5SC9","executionInfo":{"status":"ok","timestamp":1718242526739,"user_tz":-480,"elapsed":1457,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[],"source":["from rouge_score import rouge_scorer\n","scorer = rouge_scorer.RougeScorer(['rougeL'])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j05JtmNp-jx4","outputId":"ed21a1c0-50e1-46cf-b662-288d472da881","executionInfo":{"status":"ok","timestamp":1718242536138,"user_tz":-480,"elapsed":521,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.16987179487179488\n"]}],"source":["reference = \"\"\"We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection. Furthermore, we apply our model to prune the self-labeled training data.\"\"\"\n","\n","print(scorer.score(reference, predicted)['rougeL'].fmeasure)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"Nv4TqHHT5Qsi","outputId":"6372000d-c1cc-42df-9552-d027b94ce282","executionInfo":{"status":"ok","timestamp":1718242540697,"user_tz":-480,"elapsed":468,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\n\\n```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\\nThis paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\\nWe analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\\nFurthermore, we apply our model to prune the self-labeled training data.\\nExperimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\\n\\n\\n```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\\nThis paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\\nWe analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\\nFurthermore, we apply our model to prune the self-labeled training data.\\nExperimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\\n\\n\\n```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\\nThis paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\\nWe analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\\nFurthermore, we apply our model to prune the self-labeled training data.\\nExperimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\\n\\n\\n```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\\nThis paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\\nWe analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\\nFurthermore, we apply our model to prune the self-labeled training data.\\nExperimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data.```\\n\\n\\n```The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.\\nThis paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.\\nWe analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.\\nFurthermore, we apply our model to prune the self-labeled training data.\\nExperimental results show that'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["predicted"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"fnk6DvZx8yZr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718244317733,"user_tz":-480,"elapsed":1773202,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"outputId":"075fab63-ef04-42db-bde3-f3da16d40d0e"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 1: 0.278164\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 2: 0.319574\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 3: 0.312012\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 4: 0.200318\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 5: 0.123779\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 6: 0.165079\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 7: 0.111801\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 8: 0.0381944\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 9: 0.0429338\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 10: 0.122271\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 11: 0.238994\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 12: 0.287443\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 13: 0.353553\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 14: 0.142248\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 15: 0.356164\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 16: 0.0675676\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 17: 0.274882\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 18: 0.384831\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 19: 0.288828\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 20: 0.088685\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 21: 0.166667\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 22: 0.142596\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 23: 0.112805\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 24: 0.14\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 25: 0.0834725\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 26: 0.346749\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 27: 0.296089\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 28: 0.323484\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 29: 0.197461\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 30: 0.649231\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 31: 0.369281\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 32: 0.0325926\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 33: 0.209106\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 34: 0.173776\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 35: 0.141667\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 36: 0.0875203\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 37: 0.0378788\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 38: 0.105622\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 39: 0.00884956\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 40: 0.177253\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 41: 0.142114\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 42: 0.258359\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 43: 0.304478\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 44: 0.382979\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 45: 0.0250896\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 46: 0.0362438\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 47: 0.0868217\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 48: 0.177858\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 49: 0.00648298\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 50: 0.045977\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 51: 0.287051\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 52: 0.288499\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 53: 0.111929\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 54: 0.0573888\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 55: 0.234206\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 56: 0.144231\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 57: 0.033264\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 58: 0.259375\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 59: 0.285714\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 60: 0.369327\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 61: 0.197183\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 62: 0.0361664\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 63: 0.255537\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 64: 0.266118\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 65: 0.0488599\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 66: 0.116\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 67: 0.067086\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 68: 0.0743034\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 69: 0.251429\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 70: 0.309963\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 71: 0.119097\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 72: 0.204633\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 73: 0.224852\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 74: 0.120623\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 75: 0.10327\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 76: 0.046133\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 77: 0.166008\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 78: 0.219653\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 79: 0.0662824\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 80: 0.204204\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 81: 0.137285\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 82: 0.106061\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 83: 0.218409\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 84: 0\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 85: 0.211356\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 86: 0.0542857\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 87: 0.105691\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 88: 0.0983607\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 89: 0.0634921\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 90: 0.389776\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 91: 0.189474\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 92: 0.18887\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 93: 0.296435\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 94: 0\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 95: 0\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 96: 0.412331\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 97: 0.33271\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 98: 0.0645161\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 99: 0.0820189\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:inference device is not set, using cuda:0, Tesla T4\n"]},{"output_type":"stream","name":"stdout","text":["Test case 100: 0.422665\n","Overall: 0.180079\n"]},{"output_type":"execute_result","data":{"text/plain":["0.1800794876756535"]},"metadata":{},"execution_count":12}],"source":["def evaluate(foo):\n","    import urllib.request\n","    test = \"https://www.cs.nccu.edu.tw/~hhhuang/courses/nlp2024/test2024.in\"\n","    gold = \"https://www.cs.nccu.edu.tw/~hhhuang/courses/nlp2024/test2024.gold\"\n","\n","    from rouge_score import rouge_scorer\n","    scorer = rouge_scorer.RougeScorer(['rougeL'])\n","\n","    total = 0\n","    cnt = 0\n","    with urllib.request.urlopen(test) as testin, \\\n","         urllib.request.urlopen(gold) as gold:\n","        for input, ref in zip(testin, gold):\n","            input = input.decode(\"utf-8\")\n","            ref = ref.decode(\"utf-8\")\n","            output = foo(input)\n","            score = scorer.score(ref, output)['rougeL'].fmeasure\n","            cnt += 1\n","            total += score\n","            print(\"Test case %d: %g\" % (cnt, score))\n","    print(\"Overall: %g\" % (total / cnt))\n","    return total / cnt\n","\n","# As your working function is `extract_sentence`, so do evaluation with the following statement\n","evaluate(extract_sentence)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}