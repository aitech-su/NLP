{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12lCoYQurHftKNXSD99ERLogIsM4zjGdO","timestamp":1680210126367}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Naive Bayes Classifier"],"metadata":{"id":"x6ZHOucvOOza"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import movie_reviews\n","nltk.download(\"movie_reviews\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uK8D8UDUCZlc","executionInfo":{"status":"ok","timestamp":1711548307007,"user_tz":-480,"elapsed":303,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"outputId":"7eae7cd9-6a8f-4274-88ce-55d0a326aff7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from collections import defaultdict, Counter\n","import math\n","import random\n","\n","train_X, train_Y = [], []\n","test_X, test_Y = [], []\n","\n","random.seed(0)\n","for polarity in movie_reviews.categories():\n","    for fid in movie_reviews.fileids(polarity):\n","        if random.randrange(5) == 0:\n","            test_X.append([w for w in movie_reviews.words(fid)])\n","            test_Y.append(polarity)\n","        else:\n","            train_X.append([w for w in movie_reviews.words(fid)])\n","            train_Y.append(polarity)\n","\n","print(train_X[0], train_Y[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dyfLD0SRChBr","executionInfo":{"status":"ok","timestamp":1711548309487,"user_tz":-480,"elapsed":2011,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"outputId":"c30cf7cf-d61e-40ed-ec23-f2bbc7ff79e2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'] neg\n"]}]},{"cell_type":"markdown","source":["## Model Construction\n","\n","$\\bar{y} = \\text{arg}\\max_{y \\in \\mathbf{y}} P(y|x) = \\text{arg}\\max_{y \\in \\mathbf{y}} P(y) \\prod_{i=1}^n \\frac{P(x_i|y)}{P(x_i)} = \\text{arg}\\max_{y \\in \\mathbf{y}} P(y) \\prod_{i=1}^n P(x_i|y)$\n","\n","$P(x_i|y)=\\frac{C(x_i, y) + k}{C(y) + |\\mathbf{y}| \\times k}$\n","\n","$\\bar{y} = \\textrm{arg} \\max_{y \\in \\mathbf{y}} \\log P(y) + \\sum_{i=1}^n \\log \\frac{C(x_i, y) + k}{C(y) + k|\\mathbf{y}|}$\n","\n","     "],"metadata":{"id":"8TiSvHHAhy6H"}},{"cell_type":"code","source":["from collections import Counter, defaultdict\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","import math\n","import re\n","\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","class NaiveBayesClassifier:\n","    def __init__(self, k=1):\n","        self.k = k\n","        self.features = set()\n","        self.class_feature_counts = defaultdict(Counter)\n","        self.class_counts = Counter()\n","        self.total = 0\n","        self.stemmer = PorterStemmer()\n","        self.total_token_counts = defaultdict(int)\n","\n","    def lemmatize_tokens(self, tokens):\n","     lemmatizer = WordNetLemmatizer()\n","     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","     return lemmatized_tokens\n","\n","    def filter_low_frequency_words(self, tokens, min_frequency):\n","      word_counts = Counter(tokens)  # 統計每個詞彙的出現次數\n","      filtered_tokens = [token for token in tokens if word_counts[token] > min_frequency]  # 過濾掉出現次數小於 min_frequency 的詞彙\n","      return filtered_tokens\n","\n","    def remove_noise(self, tokens):\n","     stop_words = set(stopwords.words('english'))  # 加載停用詞列表\n","     filtered_tokens = [token for token in tokens if token.lower() not in stop_words]  # 過濾掉停用詞\n","     filtered_tokens = [token for token in tokens if re.match(r'^[a-zA-Z]+$', token)]  # 使用正則表達式匹配單字字符\n","     return filtered_tokens\n","\n","    def handle_negation(self, text):\n","     tokens = text.split()\n","     tagged_tokens = nltk.pos_tag(tokens)  # 對文本進行詞性標註\n","     i = 0\n","     while i < len(tagged_tokens) - 1:\n","        if tagged_tokens[i][0] == \"not\" and tagged_tokens[i+1][1] == \"JJ\":  # 檢查是否是 \"not\" 後面的形容詞\n","            tokens[i+1] += \"_NEG\"  # 給形容詞添加 \"_NEG\" 標記\n","        i += 1\n","     return tokens\n","\n","    def train(self, train_X, train_Y):\n","        for tokens, label in zip(train_X, train_Y):\n","            tokens = self.remove_noise(tokens)\n","            tokens = self.lemmatize_tokens(tokens)\n","            # tokens = [self.stemmer.stem(token) for token in tokens]\n","            # tokens = self.filter_low_frequency_words(tokens, 6)\n","            # text = ' '.join(tokens)\n","            # tokens = self.handle_negation(text)\n","            self.class_counts[label] += 1\n","            self.total += 1\n","            for token in set(tokens):\n","                self.features.add(token)\n","                self.class_feature_counts[label][token] += 1\n","                self.total_token_counts[label] += 1  # Increment total token count for this class\n","        # for label, tokens in self.class_feature_counts.items():\n","        #   for tok in tokens:\n","        #     if self.class_feature_counts[label][tok] < 6:\n","        #       self.features.discard(tok)\n","        #       self.class_feature_counts[label][tok] = 0\n","\n","    def probabilities(self, token):\n","        probs = {}\n","        for cls, cls_cnt in self.class_counts.items():\n","            token_count = self.class_feature_counts[cls][token]\n","            total_token_count = self.total_token_counts[cls]\n","            probs[cls] = (token_count + self.k) / (total_token_count + len(self.features) * self.k)\n","        return probs\n","\n","    def predict(self, tokens):\n","        tokens = self.remove_noise(tokens)\n","        tokens = self.lemmatize_tokens(tokens)\n","        # tokens = [self.stemmer.stem(token) for token in tokens]\n","        # tokens = self.filter_low_frequency_words(tokens, 6)\n","        # text = ' '.join(tokens)\n","        # tokens = self.handle_negation(text)\n","        tokens = set(tokens)\n","        log_probs = Counter()\n","        for cls, cls_cnt in self.class_counts.items():\n","            log_probs[cls] = math.log(cls_cnt / self.total)\n","        for token in self.features:\n","            probs = self.probabilities(token)\n","            if token in tokens:\n","                for cls, prob in probs.items():\n","                    log_probs[cls] += math.log(prob)\n","            else:\n","                for cls, prob in probs.items():\n","                    log_probs[cls] += math.log(1.0 - prob)\n","        return max(log_probs, key=log_probs.get), log_probs\n"],"metadata":{"id":"fobAkWURDr6L","executionInfo":{"status":"ok","timestamp":1711548309488,"user_tz":-480,"elapsed":12,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a6a9276-2c9b-4c84-8431-588ac6b0f54b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Using the Model"],"metadata":{"id":"GE_Ht04wh3Tc"}},{"cell_type":"code","source":["model = NaiveBayesClassifier()\n","model.train(train_X, train_Y)"],"metadata":{"id":"5rdZQ-aJhtld","executionInfo":{"status":"ok","timestamp":1711548316879,"user_tz":-480,"elapsed":7399,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","# Taken from https://www.imdb.com/review/rw0990793/?ref_=tt_urv\n","review = \"\"\"A whimsical, often spectacular view of a future in which advances in technology dominate the world. It is well shot and although slow-moving it is intense and enjoyable throughout. The featuring of classical music to establish atmosphere works brilliantly; it provides a feeling of awe, mystery and intrigue  the same aura that Walt Disney worked in creating 'Fantasia'. The special effects, both sound and visual, are still spellbinding by the standards of today's technology. Aside from the technical pluses of the film, it stands strong as it is one of not many films out there that has something important to say about humankind, and where the human race is heading in terms of our increasing reliance on machines and our unquenchable thirst to discover. Despite an ending that is hard to understand, it is even harder to overlook this film a true cinema classic.\"\"\"\n","\n","model.predict(word_tokenize(review))"],"metadata":{"id":"aeH93xRFZ1EF","executionInfo":{"status":"ok","timestamp":1711548316880,"user_tz":-480,"elapsed":16,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66ee6c2f-16df-4434-9e34-1295e7e60817"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["('pos', Counter({'neg': -747.4105070251276, 'pos': -732.8226860441249}))"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["correct, total = 0, 0\n","\n","for x, y in zip(test_X, test_Y):\n","    prediction, _ = model.predict(x)\n","    if prediction == y:\n","        correct += 1\n","    total += 1\n","\n","print(\"%d / %d = %g\" % (correct, total, correct / total))"],"metadata":{"id":"U1lkFRJ0FKbc","executionInfo":{"status":"ok","timestamp":1711548360392,"user_tz":-480,"elapsed":43527,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e93c9dd-4faa-4814-ed1d-ff118b06aac6"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["356 / 422 = 0.843602\n"]}]},{"cell_type":"markdown","source":["## Exploring important features"],"metadata":{"id":"pmqZUqeTgjzW"}},{"cell_type":"code","source":["def prob_class_given_feature(feature, cls, model):\n","    probs = model.probabilities(feature)\n","    return probs[cls] / sum(probs.values())\n","\n","print(sorted(model.features, key=lambda t: prob_class_given_feature(t, \"pos\", model), reverse=True)[:30])\n","print(sorted(model.features, key=lambda t: prob_class_given_feature(t, \"neg\", model), reverse=True)[:30])"],"metadata":{"id":"xofcbwlXgluq","executionInfo":{"status":"ok","timestamp":1711548360392,"user_tz":-480,"elapsed":6,"user":{"displayName":"timothy su","userId":"10803643815670288940"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47436304-4afa-402e-f156-86abf4db973a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['thematic', 'dread', 'astounding', 'naval', 'turturro', 'reminder', 'kenobi', 'fascination', 'seamless', 'denial', 'en', 'keen', 'masterfully', 'lovingly', 'burbank', 'balancing', 'downside', 'timeless', 'outstanding', 'lofty', 'uncut', 'criticized', 'dewey', 'meryl', 'splash', 'deliberate', 'vocal', 'gattaca', 'topping', 'fabric']\n","['hudson', 'vomit', 'illogical', 'sans', 'overwrought', 'tedium', 'pathetically', 'horrid', 'bio', 'undermines', 'plant', 'zellweger', 'schumacher', 'hmmm', 'plodding', 'anthropologist', 'plod', 'stupidly', 'batgirl', 'campiness', 'insulting', 'biologist', 'sphere', 'guinea', 'leaden', 'chevy', 'mug', 'stalk', 'ludicrous', 'lecture']\n"]}]}]}