{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6MBVRSro5tt","outputId":"ce2d6905-fe77-4ef5-c186-c7a2e7bd8962","executionInfo":{"status":"ok","timestamp":1716277021778,"user_tz":-480,"elapsed":12672,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import nltk\n","from nltk.corpus import movie_reviews\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"movie_reviews\")\n","\n","from collections import defaultdict, Counter\n","import math\n","import random\n","\n","random.seed(0) # Don't change\n","torch.manual_seed(0)  # Don't change\n","np.random.seed(0) # Don't change\n","\n","\n","train_X, train_Y = [], []\n","test_X, test_Y = [], []\n","\n","for polarity in movie_reviews.categories():\n","    label = 0 if polarity == 'neg' else 1\n","    for fid in movie_reviews.fileids(polarity):\n","        if random.randrange(5) == 0:\n","            test_X.append([w for w in movie_reviews.words(fid)])\n","            test_Y.append(label)\n","        else:\n","            train_X.append([w for w in movie_reviews.words(fid)])\n","            train_Y.append(label)\n","\n","print(train_X[0], train_Y[0])"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'] 0\n"]}]},{"cell_type":"markdown","source":["# Assignment II\n","Doing Assignment II by modifying the following code cell.\n","Your solution should be based on feedforward neural network (FNN or MLP) with word embeddings.\n","You are free to adjust the FNN with different dimension settings, vocabulary, overfitting prevention, and so on,\n","but you can not use other architectures (e.g., CNN/RNN/Transformer or the Naive Bayes classifier from Assignment I) in this assignment.\n"],"metadata":{"id":"ds984vYgcX7V"}},{"cell_type":"code","metadata":{"id":"Y7ru6_YNo_Z-","executionInfo":{"status":"ok","timestamp":1716277021781,"user_tz":-480,"elapsed":19,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"source":["\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","EMBEDDING_DIM = 100\n","EPOCHS = 50\n","\n","class TextClassifier(nn.Module):\n","  def init_embeddings(self, vocab):\n","    self.word_to_ix = {}\n","    weights = []\n","    ix = 0\n","    for w in vocab:\n","      self.word_to_ix[w] = ix\n","      ix += 1\n","    self.vocab_size = len(self.word_to_ix)\n","    self.embeddings = nn.EmbeddingBag(self.vocab_size, EMBEDDING_DIM)\n","\n","  def __init__(self, vocab, classes):\n","    super(TextClassifier, self).__init__()\n","    self.classes = classes\n","    self.init_embeddings(vocab)\n","    self.fc1 = nn.Linear(self.embeddings.embedding_dim, 50)\n","    self.fc1.weight.data.uniform_(-0.5, 0.5)\n","    self.fc1.bias.data.zero_()\n","    self.fc2 = nn.Linear(50, 20)\n","    self.fc2.weight.data.uniform_(-0.5, 0.5)\n","    self.fc2.bias.data.zero_()\n","    self.out = nn.Linear(20, len(self.classes))\n","    self.out.weight.data.uniform_(-0.5, 0.5)\n","    self.out.bias.data.zero_()\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, inputs, offsets):\n","    embedded = self.embeddings(inputs, offsets)\n","    return self.out(self.relu(self.fc2(self.relu(self.fc1(embedded)))))\n","\n","def make_doc_vector(doc, word_to_ix):\n","  idxs = [word_to_ix[w] for w in doc if w in word_to_ix]\n","  return torch.tensor(idxs, dtype=torch.long)\n","\n","def generate_batch(batch):\n","  label = torch.tensor([entry[0] for entry in batch])\n","  text = [entry[1] for entry in batch]\n","  offsets = [0] + [len(entry) for entry in text]\n","  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","  text = torch.cat(text)\n","  return text, offsets, label\n","\n","def build_vocab(X):\n","  word_count = Counter()\n","  for x in X:\n","    for w in x:\n","      word_count[w] += 1\n","  # The order of keys in a dictionary/set is not deterministic,\n","  # so sorting in the following statement is important to avoid randomness.\n","  return [w for (w, c) in sorted(word_count.items()) if c >= 6]\n","\n","def build_model(X, Y):\n","  model = TextClassifier(build_vocab(X), [0, 1]).to(device)\n","  loss_function = nn.CrossEntropyLoss().to(device)\n","  optimizer = optim.Adam(model.parameters())\n","\n","  train_set = []\n","  yc = Counter()\n","  for x, y in zip(X, Y):\n","    entry = []\n","    yc[y] += 1\n","    entry.append(torch.LongTensor([y]))\n","    entry.append(make_doc_vector(x, model.word_to_ix))\n","    train_set.append(entry)\n","  print(yc)\n","  data = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=generate_batch)\n","\n","  for epoch in range(EPOCHS):\n","    train_loss, train_acc = 0, 0\n","    print(\"Epoch: %d\" % epoch)\n","    for _, (x, offsets, y) in enumerate(data):\n","      model.zero_grad()\n","      x, offsets, y = x.to(device), offsets.to(device), y.to(device)\n","      pred = model(x, offsets)\n","      loss = loss_function(pred, y)\n","      loss.backward()\n","      optimizer.step()\n","      train_loss += loss.item()\n","      train_acc += (pred.argmax(1) == y).sum().item()\n","    print(\"Loss: %g, Acc: %g\" % (train_loss / len(train_set), train_acc / len(train_set)))\n","  return model"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aek5tb3yulJb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4479456f-b5c7-46d8-e70b-ea17017245f9","executionInfo":{"status":"ok","timestamp":1716277111642,"user_tz":-480,"elapsed":89876,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"source":["model = build_model(train_X, train_Y)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({1: 803, 0: 775})\n","Epoch: 0\n","Loss: 0.0421303, Acc: 0.579214\n","Epoch: 1\n","Loss: 0.038291, Acc: 0.687579\n","Epoch: 2\n","Loss: 0.0318874, Acc: 0.774398\n","Epoch: 3\n","Loss: 0.0239092, Acc: 0.842205\n","Epoch: 4\n","Loss: 0.0152749, Acc: 0.922687\n","Epoch: 5\n","Loss: 0.00909809, Acc: 0.963878\n","Epoch: 6\n","Loss: 0.00535729, Acc: 0.97782\n","Epoch: 7\n","Loss: 0.00286174, Acc: 0.995564\n","Epoch: 8\n","Loss: 0.00137265, Acc: 0.998099\n","Epoch: 9\n","Loss: 0.00074278, Acc: 1\n","Epoch: 10\n","Loss: 0.00044762, Acc: 1\n","Epoch: 11\n","Loss: 0.000293346, Acc: 1\n","Epoch: 12\n","Loss: 0.000203297, Acc: 1\n","Epoch: 13\n","Loss: 0.000149962, Acc: 1\n","Epoch: 14\n","Loss: 0.000113523, Acc: 1\n","Epoch: 15\n","Loss: 8.80247e-05, Acc: 1\n","Epoch: 16\n","Loss: 6.99068e-05, Acc: 1\n","Epoch: 17\n","Loss: 5.76091e-05, Acc: 1\n","Epoch: 18\n","Loss: 4.58473e-05, Acc: 1\n","Epoch: 19\n","Loss: 3.7902e-05, Acc: 1\n","Epoch: 20\n","Loss: 3.19071e-05, Acc: 1\n","Epoch: 21\n","Loss: 2.67353e-05, Acc: 1\n","Epoch: 22\n","Loss: 2.34634e-05, Acc: 1\n","Epoch: 23\n","Loss: 2.05173e-05, Acc: 1\n","Epoch: 24\n","Loss: 1.73228e-05, Acc: 1\n","Epoch: 25\n","Loss: 1.50802e-05, Acc: 1\n","Epoch: 26\n","Loss: 1.32645e-05, Acc: 1\n","Epoch: 27\n","Loss: 1.17268e-05, Acc: 1\n","Epoch: 28\n","Loss: 1.03572e-05, Acc: 1\n","Epoch: 29\n","Loss: 9.26073e-06, Acc: 1\n","Epoch: 30\n","Loss: 8.23692e-06, Acc: 1\n","Epoch: 31\n","Loss: 7.40301e-06, Acc: 1\n","Epoch: 32\n","Loss: 6.63707e-06, Acc: 1\n","Epoch: 33\n","Loss: 5.94069e-06, Acc: 1\n","Epoch: 34\n","Loss: 5.43828e-06, Acc: 1\n","Epoch: 35\n","Loss: 4.88321e-06, Acc: 1\n","Epoch: 36\n","Loss: 4.44759e-06, Acc: 1\n","Epoch: 37\n","Loss: 4.03371e-06, Acc: 1\n","Epoch: 38\n","Loss: 3.69621e-06, Acc: 1\n","Epoch: 39\n","Loss: 3.37576e-06, Acc: 1\n","Epoch: 40\n","Loss: 3.0823e-06, Acc: 1\n","Epoch: 41\n","Loss: 2.79962e-06, Acc: 1\n","Epoch: 42\n","Loss: 2.56264e-06, Acc: 1\n","Epoch: 43\n","Loss: 2.37619e-06, Acc: 1\n","Epoch: 44\n","Loss: 2.1813e-06, Acc: 1\n","Epoch: 45\n","Loss: 1.99975e-06, Acc: 1\n","Epoch: 46\n","Loss: 1.84732e-06, Acc: 1\n","Epoch: 47\n","Loss: 1.70024e-06, Acc: 1\n","Epoch: 48\n","Loss: 1.56709e-06, Acc: 1\n","Epoch: 49\n","Loss: 1.45498e-06, Acc: 1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aT85z26VAInH","outputId":"12cbc8c3-1222-4833-9cdc-5b3c9e578f2d","executionInfo":{"status":"ok","timestamp":1716277111643,"user_tz":-480,"elapsed":31,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"source":["def predict(model, document):\n","  probs = model(make_doc_vector(document, model.word_to_ix).to(device), torch.tensor([0]).cumsum(dim=0).to(device))\n","  return int(torch.argmax(probs))\n","\n","print(predict(model, \"this is a uninteresting movie\".split(\" \")))\n","print(predict(model, \"a good movie of this year\".split(\" \")))\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zo1u7CAk6bvR"},"source":["## Do Evaluation"]},{"cell_type":"code","source":["correct, total = 0, 0\n","\n","for x, y in zip(test_X, test_Y):\n","    prediction = predict(model, x)\n","    if prediction == y:\n","        correct += 1\n","    total += 1\n","\n","print(\"%d / %d = %g\" % (correct, total, correct / total))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eph3yR9Ma3rP","outputId":"ccc3d80c-7e25-48f3-e92c-5d19b0161be8","executionInfo":{"status":"ok","timestamp":1716277111643,"user_tz":-480,"elapsed":28,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["361 / 422 = 0.85545\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0FZpFEtLjvEf","executionInfo":{"status":"ok","timestamp":1716277111643,"user_tz":-480,"elapsed":25,"user":{"displayName":"timothy su","userId":"10803643815670288940"}}},"execution_count":5,"outputs":[]}]}